{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrT_U1533MfF"
      },
      "source": [
        "#2018111034"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUy6qxCo3PZA",
        "outputId": "f025ebc8-e61b-48c4-a12d-f969e0e2f4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "!pip install gif"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gif\n",
            "  Downloading https://files.pythonhosted.org/packages/58/41/a556a028c7c04dba76ced9bbfb321ef63df48d50153d18291b433a3d9a89/gif-3.0.0.tar.gz\n",
            "Collecting Pillow>=7.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 2.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gif\n",
            "  Building wheel for gif (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gif: filename=gif-3.0.0-cp36-none-any.whl size=4816 sha256=ea1ecc0e22ad8d5119aac78b2049f683dc5cfc3e1987027015364ba14b854808\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/1a/03/e7ccc13d5cbed82b0fda53a7792dfe372cf8baf691601d78d1\n",
            "Successfully built gif\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow, gif\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed Pillow-8.0.1 gif-3.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwGWNHs2xIsx"
      },
      "source": [
        "# Logistic Regression Excercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcaBeePgu_Tu"
      },
      "source": [
        "## Multi-class classification of MNIST using Logistic Regression\n",
        "\n",
        "The multi-class scenario for logistic regression is quite similar to the binary case, except that the label $y$ is now an integer in {1, ...., K} where $K$ is the number of classes. In this excercise you will be provided with handwritten digit images. Write the code and compute the test accuracy by training a logistic regression based classifier in (i) one-vs-one, and (ii) one-vs-all setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT-UvRJW1d_9",
        "outputId": "94aa83aa-ef0c-4c3d-8ae1-3ef6a7ed24e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import io, os, sys, types\n",
        "from IPython import get_ipython\n",
        "from nbformat import read\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "def find_notebook(fullname, path=None):\n",
        "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
        "\n",
        "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
        "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
        "    does not exist.\n",
        "    \"\"\"\n",
        "    name = fullname.rsplit('.', 1)[-1]\n",
        "    if not path:\n",
        "        path = ['']\n",
        "    for d in path:\n",
        "        nb_path = os.path.join(d, name + \".ipynb\")\n",
        "        #print('searching: %s'%nb_path)\n",
        "        if os.path.isfile(nb_path):\n",
        "            return nb_path\n",
        "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
        "        nb_path = nb_path.replace(\"_\", \" \")\n",
        "        #print('searching: %s' % nb_path)\n",
        "        if os.path.isfile(nb_path):\n",
        "            return nb_path\n",
        "\n",
        "class NotebookLoader(object):\n",
        "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
        "    def __init__(self, path=None):\n",
        "        self.shell = InteractiveShell.instance()\n",
        "        self.path = path\n",
        "\n",
        "    def load_module(self, fullname):\n",
        "        \"\"\"import a notebook as a module\"\"\"\n",
        "        path = find_notebook(fullname, self.path)\n",
        "\n",
        "        print (\"importing Jupyter notebook from %s\" % path)\n",
        "\n",
        "        # load the notebook object\n",
        "        with io.open(path, 'r', encoding='utf-8') as f:\n",
        "            nb = read(f, 4)\n",
        "\n",
        "\n",
        "        # create the module and add it to sys.modules\n",
        "        # if name in sys.modules:\n",
        "        #    return sys.modules[name]\n",
        "        mod = types.ModuleType(fullname)\n",
        "        mod.__file__ = path\n",
        "        mod.__loader__ = self\n",
        "        mod.__dict__['get_ipython'] = get_ipython\n",
        "        sys.modules[fullname] = mod\n",
        "\n",
        "        # extra work to ensure that magics that would affect the user_ns\n",
        "        # actually affect the notebook module's ns\n",
        "        save_user_ns = self.shell.user_ns\n",
        "        self.shell.user_ns = mod.__dict__\n",
        "\n",
        "        #print('Found %d cells'%len(nb.cells))\n",
        "        try:\n",
        "          for cell in nb.cells:\n",
        "            if cell.cell_type == 'code':\n",
        "                # transform the input to executable Python\n",
        "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
        "                # run the code in themodule\n",
        "                exec(code, mod.__dict__)\n",
        "        finally:\n",
        "            self.shell.user_ns = save_user_ns\n",
        "        return mod\n",
        "\n",
        "class NotebookFinder(object):\n",
        "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
        "    def __init__(self):\n",
        "        self.loaders = {}\n",
        "\n",
        "    def find_module(self, fullname, path=None):\n",
        "        nb_path = find_notebook(fullname, path)\n",
        "        if not nb_path:\n",
        "            return\n",
        "\n",
        "        key = path\n",
        "        if path:\n",
        "            # lists aren't hashable\n",
        "            key = os.path.sep.join(path)\n",
        "\n",
        "        if key not in self.loaders:\n",
        "            self.loaders[key] = NotebookLoader(path)\n",
        "        return self.loaders[key]\n",
        "\n",
        "\n",
        "#  register the NotebookFinder with sys.meta_path\n",
        "print('running importer')\n",
        "sys.meta_path.append(NotebookFinder())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running importer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ManRVu7IsIjp",
        "outputId": "35b42725-43de-4842-c108-8626826a0394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns; sns.set();\n",
        "import pandas as pd\n",
        "from utils import plot_decision_boundary, get_accuracy, get_prediction\n",
        "from utils import plot_2D_input_datapoints, generate_gifs, sigmoid, normalize\n",
        "import math\n",
        "import gif\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from utils.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbV2U06Cs45b"
      },
      "source": [
        "# Let's initialize our weights using uniform distribution\n",
        "def weight_init_uniform_dist(X, y):\n",
        "  \n",
        "    np.random.seed(312)\n",
        "    n_samples, n_features = np.shape(X)\n",
        "    _, n_outputs = np.shape(y)\n",
        "\n",
        "    limit = 1 / math.sqrt(n_features)\n",
        "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
        "    weights[-1] = 0\n",
        "\n",
        "    return weights"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAAbK03fLCR1"
      },
      "source": [
        "np.random.seed(12)\n",
        "\n",
        "# One hot encoding of our output label vector y\n",
        "def one_hot(a):\n",
        "    b = np.zeros((a.size, a.max()+1))\n",
        "    b[np.arange(a.size), a] = 1\n",
        "    return b\n",
        "\n",
        "# Loading dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# One-hot encoding of target label, Y\n",
        "Y = digits.target\n",
        "Y = one_hot(Y)\n",
        "\n",
        "# Absorbing weight b of the hyperplane\n",
        "X = digits.data\n",
        "b_ones = np.ones((len(X), 1))\n",
        "X = np.hstack((X, b_ones))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzdjTbEYLvPK",
        "outputId": "0c8d2ac3-b580-4817-8263-ac190f7d2a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "sns.reset_orig()\n",
        "\n",
        "plt.gray()\n",
        "plt.matshow(digits.images[10])\n",
        "plt.show();"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAALwElEQVR4nO3d34tc9RnH8c/HNUFrYhaiFTViLJSACN0ECRVF2oRIrBK96EUCFVZa0otWDA2I9qbJPyDpRRFC1ASMEY0GirTWgFlEaLVJXGvMxmJCxAR1/UFI4kWD5unFnJR02XbPrud7Znae9wuGzM7OnOfZ3XzmnDNz5jyOCAHob5d0uwEA5RF0IAGCDiRA0IEECDqQAEEHEuiJoNtebft92x/YfrRwradsj9s+VLLORfVusL3P9mHb79l+uHC9y2y/Zfudqt7mkvWqmgO237b9culaVb3jtt+1PWp7f+Fag7Z32z5ie8z2bQVrLal+pguX07Y3NLLwiOjqRdKApKOSvidprqR3JN1csN6dkpZJOtTSz3etpGXV9fmS/ln457OkedX1OZLelPTDwj/jbyQ9K+nlln6nxyVd1VKtHZJ+UV2fK2mwpboDkj6RdGMTy+uFNfpySR9ExLGIOCfpOUn3lSoWEa9L+rLU8iep93FEHKyun5E0Jun6gvUiIs5WX86pLsWOirK9SNI9kraVqtEttheos2J4UpIi4lxEnGqp/EpJRyPiwyYW1gtBv17SRxd9fUIFg9BNthdLWqrOWrZknQHbo5LGJe2NiJL1tkh6RNL5gjUmCkmv2j5ge33BOjdJ+kzS09WuyTbbVxSsd7G1knY1tbBeCHoKtudJelHShog4XbJWRHwTEUOSFklabvuWEnVs3ytpPCIOlFj+/3FHRCyTdLekX9m+s1CdS9XZzXsiIpZK+kpS0deQJMn2XElrJL3Q1DJ7IegnJd1w0deLqtv6hu056oR8Z0S81FbdajNzn6TVhUrcLmmN7ePq7HKtsP1MoVr/EREnq3/HJe1RZ/evhBOSTly0RbRbneCXdrekgxHxaVML7IWg/13S923fVD2TrZX0xy731BjbVmcfbywiHm+h3tW2B6vrl0taJelIiVoR8VhELIqIxer83V6LiJ+VqHWB7Stsz79wXdJdkoq8gxIRn0j6yPaS6qaVkg6XqDXBOjW42S51Nk26KiK+tv1rSX9R55XGpyLivVL1bO+S9CNJV9k+Iel3EfFkqXrqrPUekPRutd8sSb+NiD8VqnetpB22B9R5In8+Ilp526sl10ja03n+1KWSno2IVwrWe0jSzmoldEzSgwVrXXjyWiXpl40ut3opH0Af64VNdwCFEXQgAYIOJEDQgQQIOpBATwW98OGMXatFPep1u15PBV1Sm7/MVv9w1KNeN+v1WtABFFDkgBnbfX0UzsDAwLQfc/78eV1yycyeV6+77rppP+bs2bOaN2/ejOotXLhw2o/54osvZvQ4STpz5sy0H3P69GldeeWVM6p39OjRGT1utogIT7yt64fAzkbz589vtd7GjRtbrTc8PNxqvZGRkVbr3X///a3W6wVsugMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSKBW0NscmQSgeVMGvTrJ4B/UOQXtzZLW2b65dGMAmlNnjd7qyCQAzasT9DQjk4B+1diHWqoPyrf9mV0ANdQJeq2RSRGxVdJWqf8/pgrMNnU23ft6ZBKQwZRr9LZHJgFoXq199GpOWKlZYQAK48g4IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJMKllBrZv395qvfvua/dTwZs3b261XtuTYdqu1/b/l8mwRgcSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACdUYyPWV73PahNhoC0Lw6a/TtklYX7gNAQVMGPSJel/RlC70AKIR9dCABZq8BCTQWdGavAb2LTXcggTpvr+2S9FdJS2yfsP3z8m0BaFKdIYvr2mgEQDlsugMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSKAvZq8tXry41Xptz0LbsWNHq/U2bdrUar3BwcFW6w0NDbVarxewRgcSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACdU4OeYPtfbYP237P9sNtNAagOXWOdf9a0saIOGh7vqQDtvdGxOHCvQFoSJ3Zax9HxMHq+hlJY5KuL90YgOZMax/d9mJJSyW9WaIZAGXU/piq7XmSXpS0ISJOT/J9Zq8BPapW0G3PUSfkOyPipcnuw+w1oHfVedXdkp6UNBYRj5dvCUDT6uyj3y7pAUkrbI9Wl58U7gtAg+rMXntDklvoBUAhHBkHJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCCBvpi9durUqW63UNT27du73UJR/f736wWs0YEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpBAnbPAXmb7LdvvVLPXNrfRGIDm1DnW/V+SVkTE2er87m/Y/nNE/K1wbwAaUucssCHpbPXlnOrCgAZgFqm1j257wPaopHFJeyOC2WvALFIr6BHxTUQMSVokabntWybex/Z62/tt72+6SQDfzrRedY+IU5L2SVo9yfe2RsStEXFrU80BaEadV92vtj1YXb9c0ipJR0o3BqA5dV51v1bSDtsD6jwxPB8RL5dtC0CT6rzq/g9JS1voBUAhHBkHJEDQgQQIOpAAQQcSIOhAAgQdSICgAwkQdCCBvpi9NjQ01O0WgJ7GGh1IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJ1A56NcThbducGBKYZaazRn9Y0lipRgCUU3ck0yJJ90jaVrYdACXUXaNvkfSIpPMFewFQSJ1JLfdKGo+IA1Pcj9lrQI+qs0a/XdIa28clPSdphe1nJt6J2WtA75oy6BHxWEQsiojFktZKei0ifla8MwCN4X10IIFpnUoqIkYkjRTpBEAxrNGBBAg6kABBBxIg6EACBB1IgKADCRB0IAGCDiTQF7PXRkdHu91CUQsWLGi13uDgYKv12p6dt2nTplbr9QLW6EACBB1IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUig1iGw1amez0j6RtLXnNIZmF2mc6z7jyPi82KdACiGTXcggbpBD0mv2j5ge33JhgA0r+6m+x0RcdL2dyXttX0kIl6/+A7VEwBPAkAPqrVGj4iT1b/jkvZIWj7JfZi9BvSoOtNUr7A9/8J1SXdJOlS6MQDNqbPpfo2kPbYv3P/ZiHilaFcAGjVl0CPimKQftNALgEJ4ew1IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAKOiOYXaje/0B4yMjLS7RaKOn78eLdbKGp4eLjbLRQVEZ54G2t0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJFAr6LYHbe+2fcT2mO3bSjcGoDl1Bzj8XtIrEfFT23MlfadgTwAaNmXQbS+QdKekYUmKiHOSzpVtC0CT6my63yTpM0lP237b9rZqkMN/sb3e9n7b+xvvEsC3Uifol0paJumJiFgq6StJj068EyOZgN5VJ+gnJJ2IiDerr3erE3wAs8SUQY+ITyR9ZHtJddNKSYeLdgWgUXVfdX9I0s7qFfdjkh4s1xKAptUKekSMSmLfG5ilODIOSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACzF6bgcHBwVbrbdmypdV6Q0NDrdZrexba6Ohoq/Xaxuw1ICmCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQggSmDbnuJ7dGLLqdtb2ijOQDNmPKccRHxvqQhSbI9IOmkpD2F+wLQoOluuq+UdDQiPizRDIAyphv0tZJ2lWgEQDm1g16d032NpBf+x/eZvQb0qLoDHCTpbkkHI+LTyb4ZEVslbZX6/2OqwGwznU33dWKzHZiVagW9GpO8StJLZdsBUELdkUxfSVpYuBcAhXBkHJAAQQcSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kECp2WufSZrJZ9avkvR5w+30Qi3qUa+tejdGxNUTbywS9JmyvT8ibu23WtSjXrfrsekOJEDQgQR6Lehb+7QW9ajX1Xo9tY8OoIxeW6MDKICgAwkQdCABgg4kQNCBBP4NCzV9vYiL0lkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CIYTv4x65As",
        "outputId": "71aaab09-ecc2-499e-b97f-329c67bb08e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Splitting dataset into train, val, and test set.\n",
        "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.167)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
        "\n",
        "print(\"Training dataset: \", X_train.shape)\n",
        "print(\"Validation dataset: \", X_val.shape)\n",
        "print(\"Test dataset: \", X_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset:  (1308, 65)\n",
            "Validation dataset:  (188, 65)\n",
            "Test dataset:  (301, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3NzkO4s68RX"
      },
      "source": [
        "# Normalizing X_train and absorbing weight b of the hyperplane\n",
        "X_normalized_train = normalize(X_train[:, :64])\n",
        "\n",
        "b_ones = np.ones((len(X_normalized_train), 1))\n",
        "X_normalized_train = np.hstack((X_normalized_train, b_ones))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYrK4fK3iyyk",
        "outputId": "48634469-4e24-456f-c274-bf0431b82ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_normalized_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1308, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oseIo7nk1eBc"
      },
      "source": [
        "### Write your code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TC1vEE01eBe"
      },
      "source": [
        "\n",
        "def train(X, Y, epochs=1000, lr=0.01):\n",
        "    n_samples, n_features = X.shape\n",
        "    const = 1 / np.sqrt(n_features)\n",
        "    W = np.random.randn(n_features, 1) * const\n",
        "    W[-1, :] = 0\n",
        "    for epoch in range(epochs):\n",
        "        Y_pred = sigmoid(np.dot(X , W))\n",
        "        gradient = np.dot(1 / n_samples * X.T , (Y - Y_pred))\n",
        "        W += lr * gradient\n",
        "        if epoch % (epochs // 5) == 0:\n",
        "            Y_pred = np.where(Y == 1, Y_pred, 1 - Y_pred)\n",
        "            cost = -1 / Y.shape[0] * np.sum(np.log(Y_pred))\n",
        "            print(f\"Epoch {epoch}: train loss {cost}\")\n",
        "    return W\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggN1JkKX3tHi"
      },
      "source": [
        "#OneVSOne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7AmpQq13gAo",
        "outputId": "d6a8be73-b5ca-474f-e473-746fbc0fcf36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def onevsoneClassifier(X_train, Y_train, epochs=1000, lr=0.01):\n",
        "    n_examples, n_features = X_train.shape\n",
        "    _, n_classes = Y_train.shape\n",
        "    \n",
        "    W = []\n",
        "    print(f\"Training one vs one classifier starts\")\n",
        "    for k1 in range(n_classes):\n",
        "        for k2 in range(n_classes):\n",
        "            print(f\"Training {k1} vs {k2} classifier\")\n",
        "            idx = np.where((Y_train[:, k1] == 1) | (Y_train[:, k2] == 1))\n",
        "            X = X_train[idx]\n",
        "            Y = Y_train[:, k1:k1+1][idx]\n",
        "            W.append(train(X, Y, epochs, lr).ravel())\n",
        "    print(f\"Training one vs one classifier ends\")\n",
        "    return np.array(W).T\n",
        "    \n",
        "W_onevsone = onevsoneClassifier(X_normalized_train, Y_train)\n",
        "W_onevsone.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training one vs one classifier starts\n",
            "Training 0 vs 0 classifier\n",
            "Epoch 0: train loss 0.6805310212344258\n",
            "Epoch 200: train loss 0.24627001934011258\n",
            "Epoch 400: train loss 0.13817201211629918\n",
            "Epoch 600: train loss 0.0940471988150826\n",
            "Epoch 800: train loss 0.07074130268291665\n",
            "Training 0 vs 1 classifier\n",
            "Epoch 0: train loss 0.6781493690660565\n",
            "Epoch 200: train loss 0.6234937178280189\n",
            "Epoch 400: train loss 0.5755938537514068\n",
            "Epoch 600: train loss 0.5334333709736282\n",
            "Epoch 800: train loss 0.4962110542512024\n",
            "Training 0 vs 2 classifier\n",
            "Epoch 0: train loss 0.6789081865078516\n",
            "Epoch 200: train loss 0.6309898757682858\n",
            "Epoch 400: train loss 0.5896396871034589\n",
            "Epoch 600: train loss 0.5527177938631298\n",
            "Epoch 800: train loss 0.5194816177003921\n",
            "Training 0 vs 3 classifier\n",
            "Epoch 0: train loss 0.7077230312620243\n",
            "Epoch 200: train loss 0.6621161181558741\n",
            "Epoch 400: train loss 0.6208335565079475\n",
            "Epoch 600: train loss 0.5833547802211768\n",
            "Epoch 800: train loss 0.5492830615318092\n",
            "Training 0 vs 4 classifier\n",
            "Epoch 0: train loss 0.687414768124301\n",
            "Epoch 200: train loss 0.6420205496087635\n",
            "Epoch 400: train loss 0.6035780023707423\n",
            "Epoch 600: train loss 0.5691278582339935\n",
            "Epoch 800: train loss 0.5378759677213922\n",
            "Training 0 vs 5 classifier\n",
            "Epoch 0: train loss 0.699903971731579\n",
            "Epoch 200: train loss 0.6610064186662511\n",
            "Epoch 400: train loss 0.6253669905310795\n",
            "Epoch 600: train loss 0.5926924491323675\n",
            "Epoch 800: train loss 0.562708978882233\n",
            "Training 0 vs 6 classifier\n",
            "Epoch 0: train loss 0.6604768176597593\n",
            "Epoch 200: train loss 0.6240558104185341\n",
            "Epoch 400: train loss 0.5909065545473964\n",
            "Epoch 600: train loss 0.5605470282146601\n",
            "Epoch 800: train loss 0.5326856619148397\n",
            "Training 0 vs 7 classifier\n",
            "Epoch 0: train loss 0.6922695953743196\n",
            "Epoch 200: train loss 0.6361796135474078\n",
            "Epoch 400: train loss 0.5866587006244984\n",
            "Epoch 600: train loss 0.5428787763434602\n",
            "Epoch 800: train loss 0.504096176023679\n",
            "Training 0 vs 8 classifier\n",
            "Epoch 0: train loss 0.7071264285449811\n",
            "Epoch 200: train loss 0.6700572531915543\n",
            "Epoch 400: train loss 0.6370700259868302\n",
            "Epoch 600: train loss 0.6066901545231834\n",
            "Epoch 800: train loss 0.5785216424521824\n",
            "Training 0 vs 9 classifier\n",
            "Epoch 0: train loss 0.6809103762756654\n",
            "Epoch 200: train loss 0.6525967496726283\n",
            "Epoch 400: train loss 0.6260726404602388\n",
            "Epoch 600: train loss 0.6011752474298374\n",
            "Epoch 800: train loss 0.577789194168205\n",
            "Training 1 vs 0 classifier\n",
            "Epoch 0: train loss 0.6877505600475057\n",
            "Epoch 200: train loss 0.6294221569514109\n",
            "Epoch 400: train loss 0.5806392476136956\n",
            "Epoch 600: train loss 0.5381116256463577\n",
            "Epoch 800: train loss 0.5006291908900684\n",
            "Training 1 vs 1 classifier\n",
            "Epoch 0: train loss 0.6825886693279242\n",
            "Epoch 200: train loss 0.2586677519869354\n",
            "Epoch 400: train loss 0.14689220589147967\n",
            "Epoch 600: train loss 0.1004247898990109\n",
            "Epoch 800: train loss 0.07569637289202356\n",
            "Training 1 vs 2 classifier\n",
            "Epoch 0: train loss 0.6883762279174467\n",
            "Epoch 200: train loss 0.6637331239871992\n",
            "Epoch 400: train loss 0.6407183723381386\n",
            "Epoch 600: train loss 0.6191541157835158\n",
            "Epoch 800: train loss 0.5989236610194326\n",
            "Training 1 vs 3 classifier\n",
            "Epoch 0: train loss 0.6740722940836463\n",
            "Epoch 200: train loss 0.643897178614485\n",
            "Epoch 400: train loss 0.6167029718532432\n",
            "Epoch 600: train loss 0.5916071225253741\n",
            "Epoch 800: train loss 0.5683241212509051\n",
            "Training 1 vs 4 classifier\n",
            "Epoch 0: train loss 0.7064727945697947\n",
            "Epoch 200: train loss 0.6800148681600077\n",
            "Epoch 400: train loss 0.6553541198350574\n",
            "Epoch 600: train loss 0.6322172056585499\n",
            "Epoch 800: train loss 0.6104772320700516\n",
            "Training 1 vs 5 classifier\n",
            "Epoch 0: train loss 0.7142024138117015\n",
            "Epoch 200: train loss 0.6785537295239785\n",
            "Epoch 400: train loss 0.6458640524272373\n",
            "Epoch 600: train loss 0.615675348473942\n",
            "Epoch 800: train loss 0.5877447991864015\n",
            "Training 1 vs 6 classifier\n",
            "Epoch 0: train loss 0.7036898726173506\n",
            "Epoch 200: train loss 0.6646047350580585\n",
            "Epoch 400: train loss 0.6295549231501164\n",
            "Epoch 600: train loss 0.5976884677424162\n",
            "Epoch 800: train loss 0.5686113321732595\n",
            "Training 1 vs 7 classifier\n",
            "Epoch 0: train loss 0.7196094088849962\n",
            "Epoch 200: train loss 0.6805237475412171\n",
            "Epoch 400: train loss 0.6500649891108929\n",
            "Epoch 600: train loss 0.6227028764407572\n",
            "Epoch 800: train loss 0.5973412557584479\n",
            "Training 1 vs 8 classifier\n",
            "Epoch 0: train loss 0.6879453192319638\n",
            "Epoch 200: train loss 0.6713619815394097\n",
            "Epoch 400: train loss 0.6577372398818381\n",
            "Epoch 600: train loss 0.6450267555559627\n",
            "Epoch 800: train loss 0.6328653042352004\n",
            "Training 1 vs 9 classifier\n",
            "Epoch 0: train loss 0.6934663371163937\n",
            "Epoch 200: train loss 0.6568905016521915\n",
            "Epoch 400: train loss 0.626108292965747\n",
            "Epoch 600: train loss 0.5983694894205809\n",
            "Epoch 800: train loss 0.5729959420036408\n",
            "Training 2 vs 0 classifier\n",
            "Epoch 0: train loss 0.7048608027977045\n",
            "Epoch 200: train loss 0.6557264709486562\n",
            "Epoch 400: train loss 0.6121071648956065\n",
            "Epoch 600: train loss 0.5729654995016793\n",
            "Epoch 800: train loss 0.5377270336416811\n",
            "Training 2 vs 1 classifier\n",
            "Epoch 0: train loss 0.7085361160337944\n",
            "Epoch 200: train loss 0.6824503916374359\n",
            "Epoch 400: train loss 0.6581172490950479\n",
            "Epoch 600: train loss 0.6353396641456902\n",
            "Epoch 800: train loss 0.6139926987909531\n",
            "Training 2 vs 2 classifier\n",
            "Epoch 0: train loss 0.6750101974679528\n",
            "Epoch 200: train loss 0.2546777371966095\n",
            "Epoch 400: train loss 0.14454583446952815\n",
            "Epoch 600: train loss 0.09880777058694326\n",
            "Epoch 800: train loss 0.07447254138994311\n",
            "Training 2 vs 3 classifier\n",
            "Epoch 0: train loss 0.6709589224082654\n",
            "Epoch 200: train loss 0.6503145601717759\n",
            "Epoch 400: train loss 0.6309688223683785\n",
            "Epoch 600: train loss 0.6126498077425393\n",
            "Epoch 800: train loss 0.5952646579933859\n",
            "Training 2 vs 4 classifier\n",
            "Epoch 0: train loss 0.686943989118347\n",
            "Epoch 200: train loss 0.6337779580310481\n",
            "Epoch 400: train loss 0.5869760697350325\n",
            "Epoch 600: train loss 0.5455906663133403\n",
            "Epoch 800: train loss 0.5088891045449468\n",
            "Training 2 vs 5 classifier\n",
            "Epoch 0: train loss 0.6999916769413328\n",
            "Epoch 200: train loss 0.666078959560454\n",
            "Epoch 400: train loss 0.6346805810406975\n",
            "Epoch 600: train loss 0.6055288281498241\n",
            "Epoch 800: train loss 0.5784372270811277\n",
            "Training 2 vs 6 classifier\n",
            "Epoch 0: train loss 0.6980381305453828\n",
            "Epoch 200: train loss 0.654804851413892\n",
            "Epoch 400: train loss 0.6166924412331849\n",
            "Epoch 600: train loss 0.5821285811483465\n",
            "Epoch 800: train loss 0.5505864101913275\n",
            "Training 2 vs 7 classifier\n",
            "Epoch 0: train loss 0.7010423933770389\n",
            "Epoch 200: train loss 0.6581033358434869\n",
            "Epoch 400: train loss 0.6226533570848768\n",
            "Epoch 600: train loss 0.5908748446150309\n",
            "Epoch 800: train loss 0.5618580547690851\n",
            "Training 2 vs 8 classifier\n",
            "Epoch 0: train loss 0.7009772607444025\n",
            "Epoch 200: train loss 0.6811240073456934\n",
            "Epoch 400: train loss 0.6624164292520826\n",
            "Epoch 600: train loss 0.6446240305445926\n",
            "Epoch 800: train loss 0.6276701505230828\n",
            "Training 2 vs 9 classifier\n",
            "Epoch 0: train loss 0.7143438595180017\n",
            "Epoch 200: train loss 0.6803179600429268\n",
            "Epoch 400: train loss 0.6488568530709964\n",
            "Epoch 600: train loss 0.6197195899519236\n",
            "Epoch 800: train loss 0.5927128309076286\n",
            "Training 3 vs 0 classifier\n",
            "Epoch 0: train loss 0.6921859006201229\n",
            "Epoch 200: train loss 0.647636208206195\n",
            "Epoch 400: train loss 0.6075546419028689\n",
            "Epoch 600: train loss 0.5712005398967782\n",
            "Epoch 800: train loss 0.5381485378067936\n",
            "Training 3 vs 1 classifier\n",
            "Epoch 0: train loss 0.687025741138013\n",
            "Epoch 200: train loss 0.6559735160443777\n",
            "Epoch 400: train loss 0.6276725814235159\n",
            "Epoch 600: train loss 0.601520638098699\n",
            "Epoch 800: train loss 0.5772731356236311\n",
            "Training 3 vs 2 classifier\n",
            "Epoch 0: train loss 0.6884348021816034\n",
            "Epoch 200: train loss 0.6661240532649774\n",
            "Epoch 400: train loss 0.6457001305088521\n",
            "Epoch 600: train loss 0.6264548067390396\n",
            "Epoch 800: train loss 0.6082191618692628\n",
            "Training 3 vs 3 classifier\n",
            "Epoch 0: train loss 0.7309875336297958\n",
            "Epoch 200: train loss 0.2636260532896122\n",
            "Epoch 400: train loss 0.14681408619847308\n",
            "Epoch 600: train loss 0.09947717270125\n",
            "Epoch 800: train loss 0.07461105022063927\n",
            "Training 3 vs 4 classifier\n",
            "Epoch 0: train loss 0.6862639226626641\n",
            "Epoch 200: train loss 0.6287092510301259\n",
            "Epoch 400: train loss 0.5786536333468671\n",
            "Epoch 600: train loss 0.5347509476032101\n",
            "Epoch 800: train loss 0.4960967611405334\n",
            "Training 3 vs 5 classifier\n",
            "Epoch 0: train loss 0.7023609169322892\n",
            "Epoch 200: train loss 0.6721950353346694\n",
            "Epoch 400: train loss 0.6449258518008057\n",
            "Epoch 600: train loss 0.6196742792621941\n",
            "Epoch 800: train loss 0.5961700574208485\n",
            "Training 3 vs 6 classifier\n",
            "Epoch 0: train loss 0.7243006934085156\n",
            "Epoch 200: train loss 0.6671222845223573\n",
            "Epoch 400: train loss 0.6170278489214988\n",
            "Epoch 600: train loss 0.5726666753000588\n",
            "Epoch 800: train loss 0.5332463271129195\n",
            "Training 3 vs 7 classifier\n",
            "Epoch 0: train loss 0.6929614654313728\n",
            "Epoch 200: train loss 0.6584881682009899\n",
            "Epoch 400: train loss 0.6267662507629085\n",
            "Epoch 600: train loss 0.5974937462692171\n",
            "Epoch 800: train loss 0.570450884537266\n",
            "Training 3 vs 8 classifier\n",
            "Epoch 0: train loss 0.6937560540519815\n",
            "Epoch 200: train loss 0.672896518386826\n",
            "Epoch 400: train loss 0.653883846885597\n",
            "Epoch 600: train loss 0.6359575638677802\n",
            "Epoch 800: train loss 0.6189464068393129\n",
            "Training 3 vs 9 classifier\n",
            "Epoch 0: train loss 0.6889328201429827\n",
            "Epoch 200: train loss 0.674640713411239\n",
            "Epoch 400: train loss 0.6611515236192362\n",
            "Epoch 600: train loss 0.6481738516948221\n",
            "Epoch 800: train loss 0.6356451438540373\n",
            "Training 4 vs 0 classifier\n",
            "Epoch 0: train loss 0.6958230181617201\n",
            "Epoch 200: train loss 0.6495456479758763\n",
            "Epoch 400: train loss 0.6104527219198388\n",
            "Epoch 600: train loss 0.5754405356079323\n",
            "Epoch 800: train loss 0.5436853652852659\n",
            "Training 4 vs 1 classifier\n",
            "Epoch 0: train loss 0.7125165648933859\n",
            "Epoch 200: train loss 0.6853610893258882\n",
            "Epoch 400: train loss 0.6604199329873267\n",
            "Epoch 600: train loss 0.6370837061206754\n",
            "Epoch 800: train loss 0.6151666426725219\n",
            "Training 4 vs 2 classifier\n",
            "Epoch 0: train loss 0.691306208109592\n",
            "Epoch 200: train loss 0.636827864515451\n",
            "Epoch 400: train loss 0.5890949224760047\n",
            "Epoch 600: train loss 0.5469518036358549\n",
            "Epoch 800: train loss 0.5096144681723562\n",
            "Training 4 vs 3 classifier\n",
            "Epoch 0: train loss 0.6943495574386613\n",
            "Epoch 200: train loss 0.6365030796751249\n",
            "Epoch 400: train loss 0.5857945272110209\n",
            "Epoch 600: train loss 0.5412525038946634\n",
            "Epoch 800: train loss 0.5020302863380672\n",
            "Training 4 vs 4 classifier\n",
            "Epoch 0: train loss 0.6162854302787871\n",
            "Epoch 200: train loss 0.24173445246116235\n",
            "Epoch 400: train loss 0.13970044836741968\n",
            "Epoch 600: train loss 0.09635410345639807\n",
            "Epoch 800: train loss 0.0730001196450434\n",
            "Training 4 vs 5 classifier\n",
            "Epoch 0: train loss 0.7258477466497399\n",
            "Epoch 200: train loss 0.6815790207607098\n",
            "Epoch 400: train loss 0.6419584361493584\n",
            "Epoch 600: train loss 0.6059823490761338\n",
            "Epoch 800: train loss 0.5731992783494281\n",
            "Training 4 vs 6 classifier\n",
            "Epoch 0: train loss 0.6974633352757943\n",
            "Epoch 200: train loss 0.6661785135723571\n",
            "Epoch 400: train loss 0.6378502462536123\n",
            "Epoch 600: train loss 0.6115185356706868\n",
            "Epoch 800: train loss 0.5869147319316387\n",
            "Training 4 vs 7 classifier\n",
            "Epoch 0: train loss 0.6868692992495993\n",
            "Epoch 200: train loss 0.6515674178481627\n",
            "Epoch 400: train loss 0.6194398264319952\n",
            "Epoch 600: train loss 0.5900460968772264\n",
            "Epoch 800: train loss 0.5631035052972257\n",
            "Training 4 vs 8 classifier\n",
            "Epoch 0: train loss 0.6953556750890366\n",
            "Epoch 200: train loss 0.661466737224347\n",
            "Epoch 400: train loss 0.6328254010056583\n",
            "Epoch 600: train loss 0.6066708027595196\n",
            "Epoch 800: train loss 0.5824042762140544\n",
            "Training 4 vs 9 classifier\n",
            "Epoch 0: train loss 0.7362894052851328\n",
            "Epoch 200: train loss 0.6809868522552921\n",
            "Epoch 400: train loss 0.6335455895061949\n",
            "Epoch 600: train loss 0.5915830196190497\n",
            "Epoch 800: train loss 0.5541774694087885\n",
            "Training 5 vs 0 classifier\n",
            "Epoch 0: train loss 0.6903931237226718\n",
            "Epoch 200: train loss 0.6524352900061271\n",
            "Epoch 400: train loss 0.6176802300672856\n",
            "Epoch 600: train loss 0.5858076932147361\n",
            "Epoch 800: train loss 0.5565450775134761\n",
            "Training 5 vs 1 classifier\n",
            "Epoch 0: train loss 0.7512955949291629\n",
            "Epoch 200: train loss 0.7128514796053486\n",
            "Epoch 400: train loss 0.6773572148163309\n",
            "Epoch 600: train loss 0.6445487705785586\n",
            "Epoch 800: train loss 0.6142082636490724\n",
            "Training 5 vs 2 classifier\n",
            "Epoch 0: train loss 0.6692437334123036\n",
            "Epoch 200: train loss 0.6373332984441968\n",
            "Epoch 400: train loss 0.6079783568122089\n",
            "Epoch 600: train loss 0.5807466712182298\n",
            "Epoch 800: train loss 0.5554306464552842\n",
            "Training 5 vs 3 classifier\n",
            "Epoch 0: train loss 0.6865220410665196\n",
            "Epoch 200: train loss 0.6563493955019446\n",
            "Epoch 400: train loss 0.629785997617507\n",
            "Epoch 600: train loss 0.6053140618717682\n",
            "Epoch 800: train loss 0.5825566558083567\n",
            "Training 5 vs 4 classifier\n",
            "Epoch 0: train loss 0.6991080058352314\n",
            "Epoch 200: train loss 0.6571769434542446\n",
            "Epoch 400: train loss 0.6198180278858192\n",
            "Epoch 600: train loss 0.5859103304667967\n",
            "Epoch 800: train loss 0.5549955330799363\n",
            "Training 5 vs 5 classifier\n",
            "Epoch 0: train loss 0.7416117858598913\n",
            "Epoch 200: train loss 0.26809761330688775\n",
            "Epoch 400: train loss 0.1491450508510304\n",
            "Epoch 600: train loss 0.10096176882206237\n",
            "Epoch 800: train loss 0.07567265809263471\n",
            "Training 5 vs 6 classifier\n",
            "Epoch 0: train loss 0.6905862653873357\n",
            "Epoch 200: train loss 0.6463027095231618\n",
            "Epoch 400: train loss 0.6086397329686181\n",
            "Epoch 600: train loss 0.5747957436244493\n",
            "Epoch 800: train loss 0.5440183510250538\n",
            "Training 5 vs 7 classifier\n",
            "Epoch 0: train loss 0.6778724421297765\n",
            "Epoch 200: train loss 0.6454939948985134\n",
            "Epoch 400: train loss 0.6161725432821406\n",
            "Epoch 600: train loss 0.5890725124774591\n",
            "Epoch 800: train loss 0.5639119605531339\n",
            "Training 5 vs 8 classifier\n",
            "Epoch 0: train loss 0.6897684901141077\n",
            "Epoch 200: train loss 0.6658612824455747\n",
            "Epoch 400: train loss 0.646347067514491\n",
            "Epoch 600: train loss 0.6282944911726298\n",
            "Epoch 800: train loss 0.6111683212203055\n",
            "Training 5 vs 9 classifier\n",
            "Epoch 0: train loss 0.6735102711464477\n",
            "Epoch 200: train loss 0.647420907315097\n",
            "Epoch 400: train loss 0.6239977821924542\n",
            "Epoch 600: train loss 0.6023694661312838\n",
            "Epoch 800: train loss 0.5822740368707828\n",
            "Training 6 vs 0 classifier\n",
            "Epoch 0: train loss 0.7047847805768137\n",
            "Epoch 200: train loss 0.6647930398682487\n",
            "Epoch 400: train loss 0.6281792218446476\n",
            "Epoch 600: train loss 0.5946423694957687\n",
            "Epoch 800: train loss 0.563900104162514\n",
            "Training 6 vs 1 classifier\n",
            "Epoch 0: train loss 0.6839371536889916\n",
            "Epoch 200: train loss 0.6470571177122382\n",
            "Epoch 400: train loss 0.6134824101211839\n",
            "Epoch 600: train loss 0.5828665734761947\n",
            "Epoch 800: train loss 0.5549108177683797\n",
            "Training 6 vs 2 classifier\n",
            "Epoch 0: train loss 0.6614789701490615\n",
            "Epoch 200: train loss 0.6227307506470822\n",
            "Epoch 400: train loss 0.5876490631915879\n",
            "Epoch 600: train loss 0.5556483148374336\n",
            "Epoch 800: train loss 0.5263891496440948\n",
            "Training 6 vs 3 classifier\n",
            "Epoch 0: train loss 0.7014752890008158\n",
            "Epoch 200: train loss 0.6440306408627503\n",
            "Epoch 400: train loss 0.5962563919528046\n",
            "Epoch 600: train loss 0.5543680824078261\n",
            "Epoch 800: train loss 0.5171786721090098\n",
            "Training 6 vs 4 classifier\n",
            "Epoch 0: train loss 0.7068057421269728\n",
            "Epoch 200: train loss 0.6759906450601959\n",
            "Epoch 400: train loss 0.6471908553807613\n",
            "Epoch 600: train loss 0.6202652383787306\n",
            "Epoch 800: train loss 0.5950819417256256\n",
            "Training 6 vs 5 classifier\n",
            "Epoch 0: train loss 0.7249442382412197\n",
            "Epoch 200: train loss 0.6803577156621655\n",
            "Epoch 400: train loss 0.6400552579289522\n",
            "Epoch 600: train loss 0.6034194402407465\n",
            "Epoch 800: train loss 0.5700572005036493\n",
            "Training 6 vs 6 classifier\n",
            "Epoch 0: train loss 0.7073939749650848\n",
            "Epoch 200: train loss 0.2545629833387873\n",
            "Epoch 400: train loss 0.1420977473219958\n",
            "Epoch 600: train loss 0.09644008733884664\n",
            "Epoch 800: train loss 0.07241201274574025\n",
            "Training 6 vs 7 classifier\n",
            "Epoch 0: train loss 0.6864005121494653\n",
            "Epoch 200: train loss 0.6290764221647626\n",
            "Epoch 400: train loss 0.57867209175033\n",
            "Epoch 600: train loss 0.5342826527616411\n",
            "Epoch 800: train loss 0.49510107733309533\n",
            "Training 6 vs 8 classifier\n",
            "Epoch 0: train loss 0.6932032069652851\n",
            "Epoch 200: train loss 0.6602737634510089\n",
            "Epoch 400: train loss 0.6321472000389237\n",
            "Epoch 600: train loss 0.6063440924013046\n",
            "Epoch 800: train loss 0.5823355039914639\n",
            "Training 6 vs 9 classifier\n",
            "Epoch 0: train loss 0.6730135243331831\n",
            "Epoch 200: train loss 0.625703466642341\n",
            "Epoch 400: train loss 0.5834828943156151\n",
            "Epoch 600: train loss 0.5456355683120414\n",
            "Epoch 800: train loss 0.5116301230343794\n",
            "Training 7 vs 0 classifier\n",
            "Epoch 0: train loss 0.6999949794398654\n",
            "Epoch 200: train loss 0.6372500797548917\n",
            "Epoch 400: train loss 0.5867462522703832\n",
            "Epoch 600: train loss 0.5429521202920862\n",
            "Epoch 800: train loss 0.5042987507537382\n",
            "Training 7 vs 1 classifier\n",
            "Epoch 0: train loss 0.6834892501477647\n",
            "Epoch 200: train loss 0.6505874055754225\n",
            "Epoch 400: train loss 0.6228129118442639\n",
            "Epoch 600: train loss 0.5973963885940203\n",
            "Epoch 800: train loss 0.5737418579840556\n",
            "Training 7 vs 2 classifier\n",
            "Epoch 0: train loss 0.7247952295474567\n",
            "Epoch 200: train loss 0.6850221319298755\n",
            "Epoch 400: train loss 0.6486474573046442\n",
            "Epoch 600: train loss 0.6153240917957439\n",
            "Epoch 800: train loss 0.5847632638109567\n",
            "Training 7 vs 3 classifier\n",
            "Epoch 0: train loss 0.6924243785615105\n",
            "Epoch 200: train loss 0.6552654731705074\n",
            "Epoch 400: train loss 0.6232525715541763\n",
            "Epoch 600: train loss 0.5940925292089207\n",
            "Epoch 800: train loss 0.5672177488457943\n",
            "Training 7 vs 4 classifier\n",
            "Epoch 0: train loss 0.7097855723282612\n",
            "Epoch 200: train loss 0.6674498918234091\n",
            "Epoch 400: train loss 0.6331692378230244\n",
            "Epoch 600: train loss 0.6025656065281622\n",
            "Epoch 800: train loss 0.5746554657309092\n",
            "Training 7 vs 5 classifier\n",
            "Epoch 0: train loss 0.7168903229669176\n",
            "Epoch 200: train loss 0.6760579886724791\n",
            "Epoch 400: train loss 0.6435327776395449\n",
            "Epoch 600: train loss 0.6142844164499696\n",
            "Epoch 800: train loss 0.5872860444573186\n",
            "Training 7 vs 6 classifier\n",
            "Epoch 0: train loss 0.6924764804532108\n",
            "Epoch 200: train loss 0.6295268100126427\n",
            "Epoch 400: train loss 0.5782020060854481\n",
            "Epoch 600: train loss 0.5337173547397417\n",
            "Epoch 800: train loss 0.49457590205604257\n",
            "Training 7 vs 7 classifier\n",
            "Epoch 0: train loss 0.7079261024157926\n",
            "Epoch 200: train loss 0.26210164694857657\n",
            "Epoch 400: train loss 0.1474365107847108\n",
            "Epoch 600: train loss 0.1003490640936003\n",
            "Epoch 800: train loss 0.07544976239466397\n",
            "Training 7 vs 8 classifier\n",
            "Epoch 0: train loss 0.6808385963088035\n",
            "Epoch 200: train loss 0.65697331695172\n",
            "Epoch 400: train loss 0.6363136534055989\n",
            "Epoch 600: train loss 0.6170274124570893\n",
            "Epoch 800: train loss 0.5987548855566815\n",
            "Training 7 vs 9 classifier\n",
            "Epoch 0: train loss 0.6984019142960449\n",
            "Epoch 200: train loss 0.658189500059505\n",
            "Epoch 400: train loss 0.6243620962554287\n",
            "Epoch 600: train loss 0.5938777654385055\n",
            "Epoch 800: train loss 0.5659967200617407\n",
            "Training 8 vs 0 classifier\n",
            "Epoch 0: train loss 0.6894707085056294\n",
            "Epoch 200: train loss 0.6549314161679727\n",
            "Epoch 400: train loss 0.6232717364122187\n",
            "Epoch 600: train loss 0.5939484000785509\n",
            "Epoch 800: train loss 0.5667245283021417\n",
            "Training 8 vs 1 classifier\n",
            "Epoch 0: train loss 0.7062375081962832\n",
            "Epoch 200: train loss 0.691367521052185\n",
            "Epoch 400: train loss 0.6772902408179219\n",
            "Epoch 600: train loss 0.6637893931865975\n",
            "Epoch 800: train loss 0.6508101681387537\n",
            "Training 8 vs 2 classifier\n",
            "Epoch 0: train loss 0.6886765315299108\n",
            "Epoch 200: train loss 0.6685271038248108\n",
            "Epoch 400: train loss 0.6502565585094735\n",
            "Epoch 600: train loss 0.6330027205620101\n",
            "Epoch 800: train loss 0.6165824073845284\n",
            "Training 8 vs 3 classifier\n",
            "Epoch 0: train loss 0.7067108900707954\n",
            "Epoch 200: train loss 0.6851297913104488\n",
            "Epoch 400: train loss 0.6654475861699382\n",
            "Epoch 600: train loss 0.6468921997590771\n",
            "Epoch 800: train loss 0.6292892788921981\n",
            "Training 8 vs 4 classifier\n",
            "Epoch 0: train loss 0.6870291981916598\n",
            "Epoch 200: train loss 0.6572592081323221\n",
            "Epoch 400: train loss 0.6295933993201873\n",
            "Epoch 600: train loss 0.6038501689814316\n",
            "Epoch 800: train loss 0.5798786172801972\n",
            "Training 8 vs 5 classifier\n",
            "Epoch 0: train loss 0.6749624749732277\n",
            "Epoch 200: train loss 0.655476913323018\n",
            "Epoch 400: train loss 0.6370190439009596\n",
            "Epoch 600: train loss 0.6194370752217057\n",
            "Epoch 800: train loss 0.602669376456144\n",
            "Training 8 vs 6 classifier\n",
            "Epoch 0: train loss 0.7019291414365088\n",
            "Epoch 200: train loss 0.6690382495836373\n",
            "Epoch 400: train loss 0.6401600576334913\n",
            "Epoch 600: train loss 0.6135296116440515\n",
            "Epoch 800: train loss 0.5887351462123411\n",
            "Training 8 vs 7 classifier\n",
            "Epoch 0: train loss 0.7112315369945151\n",
            "Epoch 200: train loss 0.6878497994154821\n",
            "Epoch 400: train loss 0.6660523646385691\n",
            "Epoch 600: train loss 0.645412926619854\n",
            "Epoch 800: train loss 0.625811381837628\n",
            "Training 8 vs 8 classifier\n",
            "Epoch 0: train loss 0.6891950544443973\n",
            "Epoch 200: train loss 0.25678613616229995\n",
            "Epoch 400: train loss 0.14496865244328233\n",
            "Epoch 600: train loss 0.09884529089192054\n",
            "Epoch 800: train loss 0.0743930531256957\n",
            "Training 8 vs 9 classifier\n",
            "Epoch 0: train loss 0.714443789369369\n",
            "Epoch 200: train loss 0.6942876215214789\n",
            "Epoch 400: train loss 0.6758480891517912\n",
            "Epoch 600: train loss 0.6584314089005496\n",
            "Epoch 800: train loss 0.6418796369684673\n",
            "Training 9 vs 0 classifier\n",
            "Epoch 0: train loss 0.7015913244203466\n",
            "Epoch 200: train loss 0.6708767607676853\n",
            "Epoch 400: train loss 0.6431111783250485\n",
            "Epoch 600: train loss 0.6172212314505584\n",
            "Epoch 800: train loss 0.5929360397557196\n",
            "Training 9 vs 1 classifier\n",
            "Epoch 0: train loss 0.7080596845129171\n",
            "Epoch 200: train loss 0.6738703902526834\n",
            "Epoch 400: train loss 0.6425573190815712\n",
            "Epoch 600: train loss 0.6138558487840794\n",
            "Epoch 800: train loss 0.587521859406449\n",
            "Training 9 vs 2 classifier\n",
            "Epoch 0: train loss 0.6804524689319014\n",
            "Epoch 200: train loss 0.6484899878912863\n",
            "Epoch 400: train loss 0.6192023399999667\n",
            "Epoch 600: train loss 0.5921171283301638\n",
            "Epoch 800: train loss 0.5670097073540897\n",
            "Training 9 vs 3 classifier\n",
            "Epoch 0: train loss 0.6878438682285875\n",
            "Epoch 200: train loss 0.6734194987200142\n",
            "Epoch 400: train loss 0.6598612121560662\n",
            "Epoch 600: train loss 0.6468274117388688\n",
            "Epoch 800: train loss 0.634247118295547\n",
            "Training 9 vs 4 classifier\n",
            "Epoch 0: train loss 0.696015398068494\n",
            "Epoch 200: train loss 0.6465512017246442\n",
            "Epoch 400: train loss 0.6030759055789866\n",
            "Epoch 600: train loss 0.5643885269261264\n",
            "Epoch 800: train loss 0.5298219245781364\n",
            "Training 9 vs 5 classifier\n",
            "Epoch 0: train loss 0.6945387810651532\n",
            "Epoch 200: train loss 0.6655939063182702\n",
            "Epoch 400: train loss 0.6409345371786663\n",
            "Epoch 600: train loss 0.6184056293537517\n",
            "Epoch 800: train loss 0.5975193032116756\n",
            "Training 9 vs 6 classifier\n",
            "Epoch 0: train loss 0.7014358196966498\n",
            "Epoch 200: train loss 0.6513236463572495\n",
            "Epoch 400: train loss 0.6064778832297671\n",
            "Epoch 600: train loss 0.5662823145790136\n",
            "Epoch 800: train loss 0.5301985342491604\n",
            "Training 9 vs 7 classifier\n",
            "Epoch 0: train loss 0.6808118170453261\n",
            "Epoch 200: train loss 0.6447941446385802\n",
            "Epoch 400: train loss 0.6124846678814346\n",
            "Epoch 600: train loss 0.5829756454545929\n",
            "Epoch 800: train loss 0.5559085454342936\n",
            "Training 9 vs 8 classifier\n",
            "Epoch 0: train loss 0.7242026345305306\n",
            "Epoch 200: train loss 0.6920111548819268\n",
            "Epoch 400: train loss 0.6714980291723075\n",
            "Epoch 600: train loss 0.6537110991605949\n",
            "Epoch 800: train loss 0.6370831706731241\n",
            "Training 9 vs 9 classifier\n",
            "Epoch 0: train loss 0.7204428907548914\n",
            "Epoch 200: train loss 0.2637047033635122\n",
            "Epoch 400: train loss 0.14762699617898314\n",
            "Epoch 600: train loss 0.10025627721940142\n",
            "Epoch 800: train loss 0.07528830751562099\n",
            "Training one vs one classifier ends\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(65, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtg83TJt3lw4",
        "outputId": "55a6e091-ab03-4561-f5df-476b542239d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def accuracy_onevsone(X, Y, W):\n",
        "    n_classes = int(np.sqrt(W.shape[1]))\n",
        "    y_pred = np.argmax(np.dot(X , W), axis = 1) // n_classes\n",
        "    return np.sum(Y[np.arange(Y.shape[0]), y_pred]) / Y.shape[0]\n",
        "\n",
        "print(f\"Training accuracy: {accuracy_onevsone(X_train, Y_train, W_onevsone)}\")\n",
        "print(f\"Validation accuracy: {accuracy_onevsone(X_val, Y_val, W_onevsone)}\")\n",
        "print(f\"Test accuracy: {accuracy_onevsone(X_test, Y_test, W_onevsone)}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.7362385321100917\n",
            "Validation accuracy: 0.75\n",
            "Test accuracy: 0.7541528239202658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNTFzwnU3xZd"
      },
      "source": [
        "#OneVSAll"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZF_QMY53rZs",
        "outputId": "76a25f9f-0083-4be1-ec1b-bcb188021083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def onevsallClassifier(X_train, Y_train, epochs=1000, lr=0.01):\n",
        "    n_examples, n_features = X_train.shape\n",
        "    _, n_classes = Y_train.shape\n",
        "    \n",
        "    W = []\n",
        "    print(f\"Training one vs all classifier starts\")\n",
        "    for k in range(n_classes):\n",
        "        print(f\"{k} vs all classifier starts\")\n",
        "        W.append(train(X_train, Y_train[:,k:k+1], epochs, lr).ravel())\n",
        "        print(f\"{k} vs all classifier ends\")\n",
        "    print(f\"Training one vs all classifier ends\")\n",
        "    return np.array(W).T\n",
        "\n",
        "W_onevsall = onevsallClassifier(X_train, Y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training one vs all classifier starts\n",
            "0 vs all classifier starts\n",
            "Epoch 0: train loss 1.664570350040058\n",
            "Epoch 200: train loss 0.012751109634733127\n",
            "Epoch 400: train loss 0.007167189652143115\n",
            "Epoch 600: train loss 0.0051393683801813205\n",
            "Epoch 800: train loss 0.00408826344654866\n",
            "0 vs all classifier ends\n",
            "1 vs all classifier starts\n",
            "Epoch 0: train loss 2.29764698407135\n",
            "Epoch 200: train loss 0.067179540840804\n",
            "Epoch 400: train loss 0.056895696498393725\n",
            "Epoch 600: train loss 0.0518750097213722\n",
            "Epoch 800: train loss 0.04867523220692286\n",
            "1 vs all classifier ends\n",
            "2 vs all classifier starts\n",
            "Epoch 0: train loss 2.6251460999090894\n",
            "Epoch 200: train loss 0.027874662914575188\n",
            "Epoch 400: train loss 0.016604970123899724\n",
            "Epoch 600: train loss 0.012075848336271124\n",
            "Epoch 800: train loss 0.009582452444725926\n",
            "2 vs all classifier ends\n",
            "3 vs all classifier starts\n",
            "Epoch 0: train loss 1.3264571479382945\n",
            "Epoch 200: train loss 0.056984945827675076\n",
            "Epoch 400: train loss 0.04158352931328251\n",
            "Epoch 600: train loss 0.03513461746767038\n",
            "Epoch 800: train loss 0.03157187744450594\n",
            "3 vs all classifier ends\n",
            "4 vs all classifier starts\n",
            "Epoch 0: train loss 4.144098825465041\n",
            "Epoch 200: train loss 0.020644033023261878\n",
            "Epoch 400: train loss 0.014456427380758027\n",
            "Epoch 600: train loss 0.011583806203663312\n",
            "Epoch 800: train loss 0.00981929992112214\n",
            "4 vs all classifier ends\n",
            "5 vs all classifier starts\n",
            "Epoch 0: train loss 11.667555330365538\n",
            "Epoch 200: train loss 0.02640768171012352\n",
            "Epoch 400: train loss 0.01799024616443412\n",
            "Epoch 600: train loss 0.014292365815536482\n",
            "Epoch 800: train loss 0.012082489641175496\n",
            "5 vs all classifier ends\n",
            "6 vs all classifier starts\n",
            "Epoch 0: train loss 4.874552970396107\n",
            "Epoch 200: train loss 0.02399276332147666\n",
            "Epoch 400: train loss 0.01598634227635805\n",
            "Epoch 600: train loss 0.012662088752560493\n",
            "Epoch 800: train loss 0.010723747463854015\n",
            "6 vs all classifier ends\n",
            "7 vs all classifier starts\n",
            "Epoch 0: train loss 1.0852939372621535\n",
            "Epoch 200: train loss 0.02932144723185051\n",
            "Epoch 400: train loss 0.018699667257848062\n",
            "Epoch 600: train loss 0.014754849988906296\n",
            "Epoch 800: train loss 0.01260049167126755\n",
            "7 vs all classifier ends\n",
            "8 vs all classifier starts\n",
            "Epoch 0: train loss 7.644120851430398\n",
            "Epoch 200: train loss 0.12714600441984994\n",
            "Epoch 400: train loss 0.10668982348729707\n",
            "Epoch 600: train loss 0.09953206762107332\n",
            "Epoch 800: train loss 0.09572283331022303\n",
            "8 vs all classifier ends\n",
            "9 vs all classifier starts\n",
            "Epoch 0: train loss 1.5960434845339724\n",
            "Epoch 200: train loss 0.0973811570346086\n",
            "Epoch 400: train loss 0.06291467572553042\n",
            "Epoch 600: train loss 0.051318005857755006\n",
            "Epoch 800: train loss 0.045608973525660346\n",
            "9 vs all classifier ends\n",
            "Training one vs all classifier ends\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkbrhn4f32x8",
        "outputId": "379ca034-1d98-497e-fe25-588cd7666f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "def accuracy_onevsall(X, Y, W):\n",
        "    y_pred = np.argmax(np.dot(X , W), axis = 1)\n",
        "    return np.sum(Y[np.arange(Y.shape[0]), y_pred]) / Y.shape[0]\n",
        "\n",
        "print(f\"Training accuracy: {accuracy_onevsall(X_train, Y_train, W_onevsall)}\")\n",
        "print(f\"Validation accuracy: {accuracy_onevsall(X_val, Y_val, W_onevsall)}\")\n",
        "print(f\"Test accuracy: {accuracy_onevsall(X_test, Y_test, W_onevsall)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.981651376146789\n",
            "Validation accuracy: 0.9521276595744681\n",
            "Test accuracy: 0.9568106312292359\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3iIjCg634d6"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}